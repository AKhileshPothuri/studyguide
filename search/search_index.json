{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Akhilesh's Study Guide","text":"<p>This website is a compilation of resources I'm using to prepare for Machine Learning and AI engineering interviews.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>ML Prep</li> <li>Neural Networks</li> <li>GenAI</li> <li>Research Papers</li> <li>Useful Links</li> </ul>"},{"location":"#about-me","title":"About Me","text":"<p>Machine Learning Engineer specializing in Generative AI. Drawing on a strong foundation in machine learning and data science, I am now focused on building and deploying innovative LLM-powered solutions and creating efficient multimodal data pipelines.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Use the navigation links above to explore different areas. I plan to add more topics over time, so check back often!</p>"},{"location":"#interview-material","title":"Interview Material","text":"<p>See the Interview PDFs and External Resources here</p>"},{"location":"genai/","title":"Generative AI","text":"<p>This section covers topics related to Generative AI models and techniques.</p>"},{"location":"genai/#llms-large-language-models","title":"LLMs (Large Language Models)","text":"<ul> <li>Transformer Architecture: [Link]</li> <li>Attention Mechanism: [Link]</li> <li>GPT Family: [Link to OpenAI documentation]</li> <li>Llama Family: [Link to Meta's Llama page]</li> <li>Gemma (Google): [Link to Google AI blog]</li> </ul>"},{"location":"genai/#fine-tuning-techniques","title":"Fine-Tuning Techniques","text":"<ul> <li>Parameter-Efficient Fine-Tuning (PEFT): [Link to Hugging Face PEFT docs]</li> <li>LoRA (Low-Rank Adaptation): [Link to LoRA paper]</li> <li>QLoRA (Quantized LoRA): [Link to QLoRA paper]</li> <li>Prompt Engineering: [Link]</li> <li>RLHF (Reinforcement Learning from Human Feedback): [Link]</li> </ul>"},{"location":"genai/#multimodal-models","title":"Multimodal Models","text":"<ul> <li>Gemini: [Link to Gemini documentation]</li> <li>Stable Diffusion: [Link to Stable Diffusion website]</li> </ul>"},{"location":"genai/#deployment-and-optimization","title":"Deployment and Optimization","text":"<ul> <li>Quantization: [Link to quantization techniques]</li> <li>Distillation: [Link to model distillation resources]</li> <li>vLLM: [Link to vLLM website]</li> </ul>"},{"location":"ml_prep/","title":"Machine Learning Preparation","text":"<p>This section contains resources for general machine learning concepts, algorithms, and data structures.</p>"},{"location":"ml_prep/#algorithms","title":"Algorithms","text":"<ul> <li>Linear Regression: [Link to a good explanation]</li> <li>Logistic Regression: [Link to another explanation]</li> <li>Decision Trees: [Link]</li> <li>Support Vector Machines (SVMs): [Link]</li> <li>K-Nearest Neighbors (KNN): [Link]</li> <li>Clustering:<ul> <li>K-Means: [Link]</li> <li>Hierarchical Clustering: [Link]</li> </ul> </li> <li>Dimensionality Reduction:<ul> <li>PCA: [Link]</li> <li>t-SNE: [Link]</li> </ul> </li> </ul>"},{"location":"ml_prep/#data-structures","title":"Data Structures","text":"<ul> <li>Arrays: [Link]</li> <li>Linked Lists: [Link]</li> <li>Trees: [Link]</li> <li>Graphs: [Link]</li> <li>Hash Tables: [Link]</li> </ul>"},{"location":"ml_prep/#key-concepts","title":"Key Concepts","text":"<ul> <li>Bias-Variance Tradeoff: [Link]</li> <li>Overfitting/Underfitting: [Link]</li> <li>Regularization: [Link]</li> <li>Cross-Validation: [Link]</li> <li>Evaluation Metrics: (Accuracy, Precision, Recall, F1-Score, AUC): [Link]</li> </ul>"},{"location":"neural_networks/","title":"Neural Networks","text":"<p>This section covers the fundamentals of neural networks and deep learning. The main goal is to have everything to prepare for the most basic interviews</p>"},{"location":"neural_networks/#foundations","title":"Foundations","text":""},{"location":"neural_networks/#what-is-a-neural-network","title":"What is a Neural Network?","text":"<p>A computational model inspired by the structure and function of biological neural networks. Consists of interconnected nodes (\"neurons\") organized in layers that process and transmit information.</p>"},{"location":"neural_networks/#key-components","title":"Key Components","text":"<ul> <li>Neurons (Nodes): The basic processing unit. Receives inputs, applies a weight and bias, and passes the result through an activation function.</li> <li>Weights: Numerical values that represent the strength of the connection between neurons. Learned during training.</li> <li>Bias: An additional parameter added to the weighted sum of inputs. Allows the neuron to activate even when all inputs are zero.</li> <li>Activation Functions: Introduce non-linearity, enabling the network to learn complex patterns.</li> <li>Layers: Organize neurons into input, hidden, and output layers.<ul> <li>Input Layer: Receives the raw input data.</li> <li>Hidden Layers: Perform the primary computations.</li> <li>Output Layer: Produces the final result (e.g., class probabilities).</li> </ul> </li> </ul>"},{"location":"neural_networks/#activation-functions","title":"Activation Functions","text":""},{"location":"neural_networks/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<ul> <li>Description: Output is 0 if input is negative, and equal to the input if positive (f(x) = max(0, x)).</li> <li>Pros: Simple, computationally efficient, helps alleviate the vanishing gradient problem.</li> <li>Cons: Can suffer from the \"dying ReLU\" problem (neurons can get stuck in an inactive state).</li> <li>Reference: Deep Sparse Rectifier Neural Networks (Glorot et al., 2011)</li> </ul>"},{"location":"neural_networks/#sigmoid","title":"Sigmoid","text":"<ul> <li>Description: Output is between 0 and 1 (f(x) = 1 / (1 + exp(-x))).</li> <li>Pros: Easy to interpret as a probability.</li> <li>Cons: Suffers from vanishing gradients (especially in deep networks), not zero-centered, computationally expensive.</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"neural_networks/#tanh-hyperbolic-tangent","title":"Tanh (Hyperbolic Tangent)","text":"<ul> <li>Description: Output is between -1 and 1 (f(x) = tanh(x)).</li> <li>Pros: Zero-centered output (can speed up learning).</li> <li>Cons: Still suffers from vanishing gradients (though less severely than sigmoid).</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"neural_networks/#softmax","title":"Softmax","text":"<ul> <li>Description: Converts a vector of numbers into a probability distribution (used in the output layer for multi-class classification).</li> <li>Pros: Outputs a valid probability distribution, suitable for multi-class problems.</li> <li>Cons: Not suitable for multi-label classification.</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"neural_networks/#loss-functions","title":"Loss Functions","text":""},{"location":"neural_networks/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<ul> <li>Description: Calculates the average squared difference between predicted and actual values (used for regression).</li> <li>Pros: Simple to calculate, differentiable.</li> <li>Cons: Sensitive to outliers.</li> <li>Reference: (Basic statistical concept)</li> </ul>"},{"location":"neural_networks/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<ul> <li>Description: Measures the difference between two probability distributions (predicted vs. actual labels) - used primarily for classification (single or multi-class)</li> <li>Pros: Well-suited for classification, penalizes incorrect predictions more strongly.</li> <li>Cons: Requires probabilistic outputs (e.g., from Softmax).</li> </ul>"},{"location":"neural_networks/#optimization-algorithms","title":"Optimization Algorithms","text":""},{"location":"neural_networks/#gradient-descent","title":"Gradient Descent","text":"<ul> <li>Description: Iteratively adjusts model parameters to minimize the loss function by moving in the direction of the negative gradient.</li> <li>Types: Batch, Stochastic, Mini-Batch<ul> <li>Batch Gradient Descent: Calculates gradient over the entire training set (slow for large datasets).</li> <li>Stochastic Gradient Descent (SGD): Calculates gradient for each individual example (noisy, can escape local minima).</li> <li>Mini-Batch Gradient Descent: Calculates gradient over a small batch of examples (balances speed and stability).</li> </ul> </li> <li>Reference: (Basic optimization concept)</li> </ul>"},{"location":"neural_networks/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<ul> <li>Description: An adaptive learning rate optimization algorithm that combines the ideas of momentum and RMSProp.</li> <li>Pros: Often converges faster and more reliably than SGD, adapts learning rates for each parameter.</li> <li>Cons: More complex than SGD, requires tuning of hyperparameters.</li> <li>Reference: Adam: A Method for Stochastic Optimization (Kingma &amp; Ba, 2014)</li> </ul>"},{"location":"neural_networks/#rmsprop-root-mean-square-propagation","title":"RMSProp (Root Mean Square Propagation)","text":"<ul> <li>Description: An adaptive learning rate method that adjusts the learning rate based on the magnitude of recent gradients.</li> <li>Pros: Handles different learning rates per parameter</li> <li>Cons: Can still make some models get out of balance and overfit</li> <li>Reference: (Hinton lecture slides - unpublished but widely known)</li> </ul>"},{"location":"neural_networks/#regularization-techniques","title":"Regularization Techniques","text":""},{"location":"neural_networks/#l1-and-l2-regularization","title":"L1 and L2 Regularization","text":"<ul> <li>Description: Add a penalty term to the loss function based on the magnitude of the weights. L1 encourages sparsity (some weights become zero), L2 shrinks weights towards zero.</li> <li>Pros: Reduces overfitting by preventing weights from becoming too large.</li> <li>Cons: Requires tuning of the regularization strength (lambda).</li> </ul>"},{"location":"neural_networks/#dropout","title":"Dropout","text":"<ul> <li>Description: Randomly sets a fraction of neuron activations to zero during training.</li> <li>Pros: Reduces overfitting by preventing neurons from becoming too reliant on specific features.</li> <li>Cons: Requires tuning of the dropout rate.</li> </ul>"},{"location":"neural_networks/#handling-imbalanced-datasets","title":"Handling Imbalanced Datasets","text":"<ul> <li>What is It: Imbalanced datasets have a disproportionate ratio of classes.</li> </ul> <p>Techniques include  1. Over Sampling 2. Use Synthetic data 3. Do Under Sampling    Reference: https://www.analyticsvidhya.com/blog/2023/09/8-techniques-to-tackle-imbalanced-data-for-classification/\"\"\"</p>"},{"location":"neural_networks/#resources-additional","title":"Resources (Additional)","text":"<ul> <li>Fast.ai Deep Learning Course: A great practical introduction to deep learning.</li> <li>3Blue1Brown Neural Network Series: Excellent visualizations and intuitive explanations on Youtube.</li> </ul>"},{"location":"neural_networks/#my-notes","title":"My Notes","text":"<p>(Add your own notes, code snippets, and explanations here.)</p> <p>You can download my interview prep notes on Nueral networks Download here</p>"},{"location":"research/","title":"Neural Networks","text":"<p>This section covers the fundamentals of neural networks and deep learning. The main goal is to have everything to prepare for the most basic interviews</p>"},{"location":"research/#foundations","title":"Foundations","text":""},{"location":"research/#what-is-a-neural-network","title":"What is a Neural Network?","text":"<p>A computational model inspired by the structure and function of biological neural networks. Consists of interconnected nodes (\"neurons\") organized in layers that process and transmit information.</p>"},{"location":"research/#key-components","title":"Key Components","text":"<ul> <li>Neurons (Nodes): The basic processing unit. Receives inputs, applies a weight and bias, and passes the result through an activation function.</li> <li>Weights: Numerical values that represent the strength of the connection between neurons. Learned during training.</li> <li>Bias: An additional parameter added to the weighted sum of inputs. Allows the neuron to activate even when all inputs are zero.</li> <li>Activation Functions: Introduce non-linearity, enabling the network to learn complex patterns.</li> <li>Layers: Organize neurons into input, hidden, and output layers.<ul> <li>Input Layer: Receives the raw input data.</li> <li>Hidden Layers: Perform the primary computations.</li> <li>Output Layer: Produces the final result (e.g., class probabilities).</li> </ul> </li> </ul>"},{"location":"research/#activation-functions","title":"Activation Functions","text":""},{"location":"research/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<ul> <li>Description: Output is 0 if input is negative, and equal to the input if positive (f(x) = max(0, x)).</li> <li>Pros: Simple, computationally efficient, helps alleviate the vanishing gradient problem.</li> <li>Cons: Can suffer from the \"dying ReLU\" problem (neurons can get stuck in an inactive state).</li> <li>Reference: Deep Sparse Rectifier Neural Networks (Glorot et al., 2011)</li> </ul>"},{"location":"research/#sigmoid","title":"Sigmoid","text":"<ul> <li>Description: Output is between 0 and 1 (f(x) = 1 / (1 + exp(-x))).</li> <li>Pros: Easy to interpret as a probability.</li> <li>Cons: Suffers from vanishing gradients (especially in deep networks), not zero-centered, computationally expensive.</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"research/#tanh-hyperbolic-tangent","title":"Tanh (Hyperbolic Tangent)","text":"<ul> <li>Description: Output is between -1 and 1 (f(x) = tanh(x)).</li> <li>Pros: Zero-centered output (can speed up learning).</li> <li>Cons: Still suffers from vanishing gradients (though less severely than sigmoid).</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"research/#softmax","title":"Softmax","text":"<ul> <li>Description: Converts a vector of numbers into a probability distribution (used in the output layer for multi-class classification).</li> <li>Pros: Outputs a valid probability distribution, suitable for multi-class problems.</li> <li>Cons: Not suitable for multi-label classification.</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"research/#loss-functions","title":"Loss Functions","text":""},{"location":"research/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<ul> <li>Description: Calculates the average squared difference between predicted and actual values (used for regression).</li> <li>Pros: Simple to calculate, differentiable.</li> <li>Cons: Sensitive to outliers.</li> <li>Reference: (Basic statistical concept)</li> </ul>"},{"location":"research/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<ul> <li>Description: Measures the difference between two probability distributions (predicted vs. actual labels) - used primarily for classification (single or multi-class)</li> <li>Pros: Well-suited for classification, penalizes incorrect predictions more strongly.</li> <li>Cons: Requires probabilistic outputs (e.g., from Softmax).</li> </ul>"},{"location":"research/#optimization-algorithms","title":"Optimization Algorithms","text":""},{"location":"research/#gradient-descent","title":"Gradient Descent","text":"<ul> <li>Description: Iteratively adjusts model parameters to minimize the loss function by moving in the direction of the negative gradient.</li> <li>Types: Batch, Stochastic, Mini-Batch<ul> <li>Batch Gradient Descent: Calculates gradient over the entire training set (slow for large datasets).</li> <li>Stochastic Gradient Descent (SGD): Calculates gradient for each individual example (noisy, can escape local minima).</li> <li>Mini-Batch Gradient Descent: Calculates gradient over a small batch of examples (balances speed and stability).</li> </ul> </li> <li>Reference: (Basic optimization concept)</li> </ul>"},{"location":"research/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<ul> <li>Description: An adaptive learning rate optimization algorithm that combines the ideas of momentum and RMSProp.</li> <li>Pros: Often converges faster and more reliably than SGD, adapts learning rates for each parameter.</li> <li>Cons: More complex than SGD, requires tuning of hyperparameters.</li> <li>Reference: Adam: A Method for Stochastic Optimization (Kingma &amp; Ba, 2014)</li> </ul>"},{"location":"research/#rmsprop-root-mean-square-propagation","title":"RMSProp (Root Mean Square Propagation)","text":"<ul> <li>Description: An adaptive learning rate method that adjusts the learning rate based on the magnitude of recent gradients.</li> <li>Pros: Handles different learning rates per parameter</li> <li>Cons: Can still make some models get out of balance and overfit</li> <li>Reference: (Hinton lecture slides - unpublished but widely known)</li> </ul>"},{"location":"research/#regularization-techniques","title":"Regularization Techniques","text":""},{"location":"research/#l1-and-l2-regularization","title":"L1 and L2 Regularization","text":"<ul> <li>Description: Add a penalty term to the loss function based on the magnitude of the weights. L1 encourages sparsity (some weights become zero), L2 shrinks weights towards zero.</li> <li>Pros: Reduces overfitting by preventing weights from becoming too large.</li> <li>Cons: Requires tuning of the regularization strength (lambda).</li> </ul>"},{"location":"research/#dropout","title":"Dropout","text":"<ul> <li>Description: Randomly sets a fraction of neuron activations to zero during training.</li> <li>Pros: Reduces overfitting by preventing neurons from becoming too reliant on specific features.</li> <li>Cons: Requires tuning of the dropout rate.</li> </ul>"},{"location":"research/#handling-imbalanced-datasets","title":"Handling Imbalanced Datasets","text":"<ul> <li>What is It: Imbalanced datasets have a disproportionate ratio of classes.</li> </ul> <p>Techniques include  1. Over Sampling 2. Use Synthetic data 3. Do Under Sampling    Reference: https://www.analyticsvidhya.com/blog/2023/09/8-techniques-to-tackle-imbalanced-data-for-classification/\"\"\"</p>"},{"location":"research/#resources-additional","title":"Resources (Additional)","text":"<ul> <li>Fast.ai Deep Learning Course: A great practical introduction to deep learning.</li> <li>3Blue1Brown Neural Network Series: Excellent visualizations and intuitive explanations on Youtube.</li> </ul>"},{"location":"research/#my-notes","title":"My Notes","text":"<p>(Add your own notes, code snippets, and explanations here.)</p> <p>You can download my interview prep notes on Nueral networks Download here</p>"}]}