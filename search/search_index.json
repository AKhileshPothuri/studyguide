{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Akhilesh's Study Guide","text":"<p>This website is a compilation of resources I'm using to prepare for Machine Learning and AI engineering interviews.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Machine Learning</li> <li>Neural Networks</li> <li>GenAI</li> <li>Research Papers</li> <li>Useful Links</li> </ul>"},{"location":"#about-me","title":"About Me","text":"<p>Machine Learning Engineer specializing in Generative AI. Drawing on a strong foundation in machine learning and data science, I am now focused on building and deploying innovative LLM-powered solutions and creating efficient multimodal data pipelines.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Use the navigation links above to explore different areas. I plan to add more topics over time, so check back often!</p>"},{"location":"#interview-material","title":"Interview Material","text":"<p>See the Interview PDFs and External Resources here</p>"},{"location":"genai/","title":"Generative AI","text":"<p>This section covers Generative AI models, fine-tuning techniques, prompting strategies, and related concepts.</p>"},{"location":"genai/#core-concepts","title":"Core Concepts","text":""},{"location":"genai/#what-is-generative-ai","title":"What is Generative AI?","text":"<p>Generative AI refers to a class of machine learning models that can generate new content, such as text, images, audio, and video. They learn the underlying patterns in training data and can create novel outputs that resemble that data.</p>"},{"location":"genai/#key-types-of-generative-models","title":"Key Types of Generative Models","text":"<ul> <li>Large Language Models (LLMs): Models trained on massive text datasets, capable of generating coherent and contextually relevant text, translations, code, and more. (Examples: GPT, Llama, Gemma, Claude).</li> <li>Diffusion Models: Used primarily for image generation. They learn to reverse a process of gradually adding noise to an image, allowing them to generate new images by starting from random noise and iteratively removing it. (Examples: Stable Diffusion, DALL-E 3, Imagen).</li> <li>Variational Autoencoders (VAEs): A type of neural network that learns a compressed, latent representation of data and can then generate new samples from that latent space.</li> <li>Generative Adversarial Networks (GANs): Consist of two networks, a generator and a discriminator, that compete against each other. The generator tries to create realistic data, while the discriminator tries to distinguish between real and generated data.</li> </ul>"},{"location":"genai/#large-language-models-llms","title":"Large Language Models (LLMs)","text":""},{"location":"genai/#transformer-architecture","title":"Transformer Architecture","text":"<ul> <li>Description: The dominant architecture for LLMs. Uses self-attention mechanisms to weigh the importance of different parts of the input sequence.</li> <li>Key Components: Attention mechanisms, multi-head attention, encoder layers, decoder layers, feedforward networks, residual connections, layer normalization.</li> <li>Key Advantages: Ability to capture long-range dependencies, parallelizable training.</li> <li>Reference: Attention is All You Need (Vaswani et al., 2017)</li> </ul>"},{"location":"genai/#attention-mechanism","title":"Attention Mechanism","text":"<ul> <li>Description: Allows the model to focus on relevant parts of the input when processing each word or token.</li> <li>Process: Calculating query, key, and value vectors; computing attention scores; weighting value vectors based on attention scores.</li> <li>Reference: Attention is All You Need (Vaswani et al., 2017)</li> </ul>"},{"location":"genai/#key-llm-families","title":"Key LLM Families","text":"<ul> <li>GPT (OpenAI): Proprietary models known for strong general-purpose language capabilities, few-shot learning, and scaling.<ul> <li>OpenAI API Documentation</li> </ul> </li> <li>Llama (Meta): Open-source models allowing more transparency, customization, and local deployment.<ul> <li>Meta Llama</li> </ul> </li> <li>Gemma (Google): Open models from Google, designed for responsible AI development and deployment. Focuses on efficiency and performance.<ul> <li>Google AI Gemma</li> </ul> </li> <li>Claude (Anthropic): Proprietary models focused on safety and helpfulness.</li> </ul>"},{"location":"genai/#fine-tuning-techniques","title":"Fine-Tuning Techniques","text":""},{"location":"genai/#supervised-fine-tuning-sft","title":"Supervised Fine-Tuning (SFT)","text":"<ul> <li>Description: Training a pre-trained LLM on a labeled dataset to adapt it to a specific task (e.g., text summarization, question answering).</li> <li>Process: Prepare a dataset of input-output pairs, train the model using a loss function that compares the predicted output to the ground truth output, adjust the model's weights to minimize the loss.</li> </ul>"},{"location":"genai/#parameter-efficient-fine-tuning-peft","title":"Parameter-Efficient Fine-Tuning (PEFT)","text":"<ul> <li>Description: Techniques to reduce the computational cost and memory requirements of fine-tuning large LLMs by only training a small subset of the model's parameters.</li> <li>Key Techniques: LoRA, QLoRA, Adapters.</li> </ul>"},{"location":"genai/#lora-low-rank-adaptation","title":"LoRA (Low-Rank Adaptation)","text":"<ul> <li>Description: Freezes the pre-trained model weights and injects trainable low-rank matrices into each Transformer layer.</li> <li>Advantages: Significantly reduces the number of trainable parameters, faster training, lower memory requirements.</li> <li>Reference: LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)</li> </ul>"},{"location":"genai/#qlora-quantized-lora","title":"QLoRA (Quantized LoRA)","text":"<ul> <li>Description: Combines LoRA with quantization techniques (e.g., 4-bit quantization) to further reduce memory footprint.</li> <li>Advantages: Extremely memory-efficient, allows fine-tuning of very large models on consumer hardware.</li> <li>Reference: QLORA: Efficient Finetuning of Quantized LLMs (Dettmers et al., 2023)</li> </ul>"},{"location":"genai/#fine-tuning-with-unsloth","title":"Fine-tuning with Unsloth","text":"<ul> <li>Description: Easy way to use QLoRA with speed improvements.<ul> <li>Code Example: <pre><code>from unsloth import FastLanguageModel  \nmodel, tokenizer = FastLanguageModel.from_pretrained(  \n    model_name=\"&lt;MODEL_NAME&gt;\", # Replace with your required data location  \n    max_seq_length=2048,  \n    load_in_4bit=True,  \n    )\n</code></pre></li> </ul> </li> </ul>"},{"location":"genai/#prompt-engineering-techniques","title":"Prompt Engineering Techniques","text":"<ul> <li>Zero-shot Prompting: Providing a prompt without any examples.</li> <li>Few-shot Prompting: Providing a prompt with a few examples to guide the model.</li> <li>Chain of Thought Prompting: Encouraging the model to reason step-by-step to improve complex reasoning tasks.<ul> <li>Reference: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022)</li> </ul> </li> <li>Pydantic will help creating and validating the inputs and outputs in Pythonic way</li> </ul>"},{"location":"genai/#multimodal-models","title":"Multimodal Models","text":""},{"location":"genai/#architectures","title":"Architectures","text":"<ul> <li>Connectors/Fusion Modules: How vision, audio, and textual representations are combined (attention mechanisms, cross-attention).</li> <li>Training Objectives: How these models are trained to align different modalities.</li> </ul>"},{"location":"genai/#models","title":"Models","text":"<ul> <li>Gemini (Google): Google Documentation</li> <li>Stable Diffusion: Stable Diffusion Website</li> </ul>"},{"location":"genai/#deployment-and-optimization","title":"Deployment and Optimization","text":""},{"location":"genai/#quantization","title":"Quantization","text":"<ul> <li>Description: Reducing the precision of model weights (e.g., from 32-bit floats to 8-bit integers) to reduce model size and improve inference speed.</li> <li>Techniques: Post-Training Quantization, Quantization-Aware Training (QAT)</li> </ul>"},{"location":"genai/#knowledge-distillation","title":"Knowledge Distillation","text":"<ul> <li>Description: Training a smaller \"student\" model to mimic the behavior of a larger \"teacher\" model.</li> <li>Pros: Reduces model size and improves inference speed.</li> </ul>"},{"location":"genai/#retrieval-augmented-generation-rag","title":"Retrieval-Augmented Generation (RAG)","text":""},{"location":"genai/#what-is-rag","title":"What is RAG?","text":"<p>Retrieval-Augmented Generation (RAG) is a technique for enhancing the knowledge and reliability of Large Language Models (LLMs) by grounding them on external sources of information.  Instead of relying solely on the information the LLM learned during its training, RAG allows the model to access and incorporate information from a vector database, knowledge graph, or other external data source.</p>"},{"location":"genai/#rag-process-overview","title":"RAG Process Overview","text":"<ol> <li>Query Encoding: The user's query is encoded into a vector embedding using an embedding model (e.g., Sentence Transformers, OpenAI embeddings).</li> <li>Document Retrieval: The encoded query vector is used to perform a similarity search in a vector database to retrieve the most relevant documents or chunks of text.</li> <li>Augmentation: The retrieved documents are prepended to the original user query, creating an augmented prompt.</li> <li>Generation: The augmented prompt is fed into the LLM, which uses the retrieved information to generate a more informed and contextually accurate response.</li> </ol>"},{"location":"genai/#benefits-of-rag","title":"Benefits of RAG","text":"<ul> <li>Improved Accuracy: Reduces hallucinations and ensures responses are grounded in factual information.</li> <li>Increased Knowledge: Extends the knowledge base of the LLM beyond its training data.</li> <li>Up-to-Date Information: Allows the LLM to incorporate real-time information and stay current.</li> <li>Explainability:  Provides the ability to trace the source of information used in the response.</li> <li>Customization: Enables adaptation to domain-specific knowledge.</li> </ul>"},{"location":"genai/#vector-databases","title":"Vector Databases","text":""},{"location":"genai/#what-are-vector-databases","title":"What are Vector Databases?","text":"<p>Vector databases are specialized databases that store and index vector embeddings, which are numerical representations of data (text, images, audio, etc.). They are designed for efficient similarity search, allowing you to quickly find the vectors that are most similar to a given query vector.</p>"},{"location":"genai/#key-concepts","title":"Key Concepts","text":"<ul> <li>Vector Embeddings:  Dense vector representations of data, capturing semantic meaning.</li> <li>Similarity Search: Algorithms for finding the nearest neighbors of a query vector in a high-dimensional space (e.g., cosine similarity, dot product).</li> <li>Indexing Techniques:  Methods for organizing vector data to enable efficient search (e.g., Approximate Nearest Neighbor (ANN) algorithms).</li> </ul>"},{"location":"genai/#popular-vector-databases","title":"Popular Vector Databases","text":"<ul> <li>Pinecone: Fully managed vector database service.<ul> <li>Pinecone Documentation</li> </ul> </li> <li>Chroma: Open-source embedding database.<ul> <li>Chroma Documentation</li> </ul> </li> <li>Weaviate: Open-source vector search engine.<ul> <li>Weaviate Documentation</li> </ul> </li> <li>Milvus: Open-source vector database built for AI applications.<ul> <li>Milvus Documentation</li> </ul> </li> </ul>"},{"location":"genai/#langchain-and-langgraph","title":"Langchain and LangGraph","text":""},{"location":"genai/#what-is-langchain","title":"What is Langchain?","text":"<p>Langchain is a framework designed to simplify the development of applications powered by large language models (LLMs). It provides a standard interface for chains, along with lots of other components that are useful for working with LLMs, such as prompt templates, indexes (for RAG), and agents.</p>"},{"location":"genai/#langchain-key-components","title":"Langchain Key Components","text":"<ul> <li>Models: Interfaces to LLMs and other models (e.g., text embedding models).</li> <li>Prompts: Templates for structuring instructions and context for LLMs.</li> <li>Chains:  Sequences of calls to LLMs or other utilities. Chains can combine multiple steps to perform a complex task.</li> <li>Indexes: Data structures for organizing and retrieving documents for RAG.  Includes vector stores, document loaders, and text splitters.</li> <li>Agents:  Systems that use an LLM to decide which actions to take.  Involve using tools (e.g., search, calculators) and iterating until a goal is achieved.</li> <li>Memory: State management tools to preserve information between calls of a Chain or Agent.</li> </ul>"},{"location":"genai/#what-is-langgraph","title":"What is LangGraph?","text":"<p>LangGraph is a library built on top of LangChain. It provides a more robust and structured way to build complex, stateful, multi-actor applications with LLMs.</p>"},{"location":"genai/#langgraph-key-features","title":"LangGraph Key Features","text":"<ul> <li>Graph Representation:  Applications are defined as a graph of interconnected nodes, where each node represents a step in the process.</li> <li>State Management: Manages the state of the application as it flows through the graph.</li> <li>Cycles and Conditional Routing:  Enables complex workflows with cycles (loops) and conditional routing based on LLM output or other criteria.</li> <li>Agentic Workflows: Facilitates the creation of agentic workflows, where multiple agents interact and collaborate to achieve a goal.</li> </ul>"},{"location":"genai/#agentic-flows","title":"Agentic Flows","text":""},{"location":"genai/#what-are-agentic-flows","title":"What are Agentic Flows?","text":"<p>Agentic flows involve using LLMs to create intelligent agents that can autonomously perform tasks. These agents can:</p> <ul> <li>Decide on Actions: Use the LLM to determine the next action to take based on the current state.</li> <li>Use Tools: Interact with external tools (e.g., search engines, APIs, databases) to gather information or perform actions.</li> <li>Observe Results: Observe the results of their actions and use that information to inform future decisions.</li> <li>Iterate:**  Repeat the process until a goal is achieved.</li> </ul>"},{"location":"genai/#key-components-of-agentic-flows","title":"Key components of Agentic Flows","text":"<ul> <li>LLM: The core decision-making engine.</li> <li>Tools: Functions or APIs that the agent can use to interact with the environment.</li> <li>Memory:  A mechanism for the agent to remember past interactions and decisions.</li> <li>Prompt: Structured language to allow to agent to decide using a LLM.</li> </ul>"},{"location":"genai/#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Autonomous Research: An agent that can research a topic, gather information from multiple sources, and synthesize a report.</li> <li>Smart Home Automation:  An agent that can control smart home devices based on user preferences and environmental conditions.</li> <li>Customer Service Chatbot: An agent that can answer customer questions, troubleshoot issues, and escalate complex cases to a human agent.</li> </ul>"},{"location":"genai/#resources","title":"Resources","text":"<ul> <li>Langchain Official Documentation: https://www.langchain.com/</li> <li>LangGraph Official Documentation: https://python.langchain.com/docs/langgraph</li> <li>Research Paper Links:<ul> <li>Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020) - Core RAG paper.</li> <li>Chain of Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022)</li> <li>ReAct: Synergizing Reasoning and Acting in Language Models (Yao et al., 2022) - ReAct is an agentic approach that creates prompts to generate both reasoning traces and actions for the language models.</li> <li>Prompt Engineering Guide</li> </ul> </li> </ul>"},{"location":"interview_prep/","title":"Interview Preparation","text":"<p>This section provides resources and example questions to help you prepare for data science and machine learning interviews.</p>"},{"location":"interview_prep/#general-resources","title":"General Resources","text":"<ul> <li>Data Science Interview Questions: A broad collection of questions covering machine learning, deep learning, and more. (Author: Steve Nouri)</li> <li>30 Day ML Interview Prep Plan: A structured guide to help you prepare for machine learning interviews in a month. (Source: Machine Learning Plus)</li> <li>A Compact Guide to Large Language Models:  Ebook on Large Language Models (LLMs) (Source: Databricks)</li> <li>Data-Driven Interview Preparation for Data Scientists: Interview preparation by 365 Data Science</li> <li>Statistical Machine Learning Notes: Statitical notes and topics to prepare for interview</li> </ul>"},{"location":"interview_prep/#sql","title":"SQL","text":"<ul> <li>SQL Cheat Sheet: A quick reference guide to SQL commands, data types, and more. (Source: Hackr.io)</li> </ul>"},{"location":"interview_prep/#ab-testing","title":"A/B Testing","text":"<ul> <li>A/B Testing Cheat Sheet: Summarized Notes for AB testing (Source: Emma Ding)</li> </ul>"},{"location":"interview_prep/#example-questions","title":"Example Questions","text":""},{"location":"interview_prep/#easy-questions","title":"Easy Questions","text":""},{"location":"interview_prep/#general","title":"General","text":"<ul> <li> <p>Q: What are the key differences between discriminative and generative models?  </p> <p>A: Discriminative models learn the decision boundary between classes (P(y|x)). Generative models learn the distribution of the data itself (P(x,y)).</p> </li> </ul>"},{"location":"interview_prep/#nlp","title":"NLP","text":"<ul> <li> <p>Q: What is the purpose of tokenization in NLP?  </p> <p>A: Tokenization is the process of breaking down text into smaller units, such as words or subwords, called tokens.</p> </li> </ul>"},{"location":"interview_prep/#python","title":"Python","text":"<ul> <li> <p>Q: How to write Python Functions  </p> <p>A: def function_name(arguments):</p> </li> </ul>"},{"location":"interview_prep/#medium-questions","title":"Medium Questions","text":""},{"location":"interview_prep/#nlp_1","title":"NLP","text":"<ul> <li>Q: Explain the concept of transfer learning in NLP. A: Transfer learning in NLP involves taking a pre-trained model on a large dataset and fine-tuning it on a smaller, task-specific dataset.</li> </ul>"},{"location":"interview_prep/#statistics","title":"Statistics","text":"<ul> <li>Q: How does quantization help in model compression? A: Quantization reduces the number of bits required to represent each weight in the model, typically from 32-bit floating-point to 8-bit integers.</li> </ul>"},{"location":"interview_prep/#python_1","title":"Python","text":"<ul> <li>Q: What are lambda expressions? A:  These are small anonymous functions defined using the <code>lambda</code> keyword.</li> </ul>"},{"location":"interview_prep/#hard-questions","title":"Hard Questions","text":""},{"location":"interview_prep/#general_1","title":"General","text":"<ul> <li>Q:  Describe the architecture of GPT and how it differs from BERT. A: GPT is a unidirectional transformer model, while BERT is a bidirectional model.</li> </ul>"},{"location":"interview_prep/#nlp_2","title":"NLP","text":"<ul> <li>Q: How would you approach a text classification problem in the Telecom domain? A: Gather and preprocess data, choose a suitable model architecture (e.g., BERT), fine-tune it, consider domain-specific embeddings, evaluate, and iterate.</li> </ul>"},{"location":"interview_prep/#statistics_1","title":"Statistics","text":"<ul> <li>Q: Explain the concept of model distillation and its benefits. A:  Model distillation trains a smaller model (\"student\") to mimic a larger model (\"teacher\") for faster inference and reduced memory usage.</li> </ul>"},{"location":"interview_prep/#additional-notes","title":"Additional Notes","text":"<p>See also: Machine Learning, Neural Networks, GenAI</p>"},{"location":"ml_prep/","title":"Machine Learning Concepts","text":"<p>This section covers fundamental and advanced concepts in machine learning.</p>"},{"location":"ml_prep/#supervised-learning","title":"Supervised Learning","text":"<p>In supervised learning, the algorithm learns from labeled data, where each example is a pair consisting of an input and a desired output. The goal is to learn a mapping function that can predict the output for new, unseen inputs.</p>"},{"location":"ml_prep/#regression","title":"Regression","text":"<p>Regression algorithms predict a continuous output variable.</p> <ul> <li>Linear Regression: Models the relationship between variables using a linear equation.<ul> <li>Concept:  Finds the best-fitting line (or hyperplane in higher dimensions) to minimize the sum of squared errors between predicted and actual values.</li> <li>Mathematical Formulation: <code>y = mx + b</code> (simple linear regression) or <code>y = Xw + b</code> (multiple linear regression).</li> <li>Cost Function: Mean Squared Error (MSE).</li> <li>Resource: StatQuest: Linear Regression (Excellent intuitive explanation)</li> <li>Scikit-learn Example: LinearRegression in scikit-learn</li> </ul> </li> <li>Polynomial Regression: Models the relationship using a polynomial equation.<ul> <li>Concept:  Extends linear regression by adding polynomial terms (e.g., <code>x^2</code>, <code>x^3</code>) to capture non-linear relationships.</li> <li>Resource: Polynomial Regression - GeeksforGeeks</li> </ul> </li> <li>Support Vector Regression (SVR):  Uses Support Vector Machines to predict continuous values.<ul> <li>Concept:  Finds a hyperplane that fits the data within a certain margin of error (epsilon).</li> <li>Resource: Support Vector Regression (SVR) - GeeksforGeeks</li> </ul> </li> </ul>"},{"location":"ml_prep/#classification","title":"Classification","text":"<p>Classification algorithms predict a categorical output variable.</p> <ul> <li>Logistic Regression:  Despite the name, used for binary classification problems.<ul> <li>Concept: Uses a sigmoid function to map the input to a probability between 0 and 1.</li> <li>Mathematical Formulation: <code>p = 1 / (1 + exp(-(Xw + b)))</code></li> <li>Cost Function: Cross-Entropy Loss (Binary Cross-Entropy).</li> <li>Resource: StatQuest: Logistic Regression</li> <li>Scikit-learn Example: LogisticRegression in scikit-learn</li> </ul> </li> <li>Decision Trees:  Creates a tree-like structure to classify data based on features.<ul> <li>Concept:  Recursively splits the data based on the feature that provides the most information gain.</li> <li>Resource: StatQuest: Decision Trees</li> <li>Scikit-learn Example: DecisionTreeClassifier in scikit-learn</li> </ul> </li> <li>Random Forests:  An ensemble method that combines multiple decision trees.<ul> <li>Concept:  Creates a collection of decision trees trained on random subsets of the data and features.</li> <li>Resource: StatQuest: Random Forests</li> <li>Scikit-learn Example: RandomForestClassifier in scikit-learn</li> </ul> </li> <li>Support Vector Machines (SVMs):  Finds the optimal hyperplane to separate data into different classes.<ul> <li>Concept:  Uses kernel functions (e.g., linear, polynomial, RBF) to map the data into a higher-dimensional space.</li> <li>Resource: SVM \u2014 Understanding the theory through math (towardsdatascience.com)</li> <li>Scikit-learn Example: SVC in scikit-learn</li> </ul> </li> <li>K-Nearest Neighbors (KNN):  Classifies data based on the majority class of its k-nearest neighbors.<ul> <li>Concept:  Calculates the distance between a new data point and all existing data points.</li> <li>Resource: K-Nearest Neighbors (KNN) - GeeksforGeeks</li> <li>Scikit-learn Example: KNeighborsClassifier in scikit-learn</li> </ul> </li> <li>Naive Bayes: A probabilistic classifier based on applying Bayes' theorem with strong (naive) independence assumptions between the features.<ul> <li>Concept: Simple and efficient, especially for text classification tasks.</li> <li>Resource: Naive Bayes classifier - Wikipedia</li> <li>Scikit-learn Example: GaussianNB in scikit-learn</li> </ul> </li> </ul>"},{"location":"ml_prep/#unsupervised-learning","title":"Unsupervised Learning","text":"<p>In unsupervised learning, the algorithm learns from unlabeled data, discovering patterns and structures within the data without explicit guidance.</p>"},{"location":"ml_prep/#clustering","title":"Clustering","text":"<p>Clustering algorithms group similar data points together.</p> <ul> <li>K-Means: Partitions the data into k clusters based on distance to cluster centroids.<ul> <li>Concept: Iteratively assigns data points to the nearest centroid and updates the centroid positions.</li> <li>Resource: StatQuest: K-means clustering</li> <li>Scikit-learn Example: KMeans in scikit-learn</li> </ul> </li> <li>Hierarchical Clustering: Creates a hierarchy of clusters, from single data points to a single cluster containing all data points.<ul> <li>Concept: Can be agglomerative (bottom-up) or divisive (top-down).</li> <li>Resource: Hierarchical Clustering - GeeksforGeeks</li> </ul> </li> <li>DBSCAN (Density-Based Spatial Clustering of Applications with Noise):  Groups data points based on density, identifying clusters of arbitrary shape.<ul> <li>Concept: Identifies core points, border points, and noise points based on neighborhood density.</li> <li>Resource: DBSCAN Clustering - Towards Data Science</li> </ul> </li> </ul>"},{"location":"ml_prep/#dimensionality-reduction","title":"Dimensionality Reduction","text":"<p>Dimensionality reduction techniques reduce the number of features in the data while preserving important information.</p> <ul> <li>Principal Component Analysis (PCA):  Transforms the data into a new coordinate system where the principal components capture the most variance.<ul> <li>Concept:  Finds the eigenvectors of the covariance matrix of the data.</li> <li>Resource: StatQuest: Principal Component Analysis (PCA) clearly explained</li> <li>Scikit-learn Example: PCA in scikit-learn</li> </ul> </li> <li>t-distributed Stochastic Neighbor Embedding (t-SNE):  Reduces dimensionality while preserving the local structure of the data, useful for visualization.<ul> <li>Concept:  Minimizes the divergence between the probability distributions of pairwise similarities in the high-dimensional and low-dimensional spaces.</li> <li>Resource: How to Use t-SNE Effectively - Towards Data Science</li> </ul> </li> </ul>"},{"location":"ml_prep/#key-concepts","title":"Key Concepts","text":"<ul> <li>Bias-Variance Tradeoff:<ul> <li>Description: Finding the right balance between a model's ability to fit the training data (low bias) and its ability to generalize to unseen data (low variance).</li> <li>Resource: Understanding the Bias-Variance Tradeoff - Towards Data Science</li> </ul> </li> <li>Overfitting/Underfitting:<ul> <li>Description: Overfitting occurs when a model learns the training data too well, resulting in poor generalization. Underfitting occurs when a model is too simple and cannot capture the underlying patterns in the data.</li> <li>Resource: Overfitting vs. Underfitting: A Conceptual Explanation - ActiveWizards</li> </ul> </li> <li>Regularization:<ul> <li>Description: Techniques to prevent overfitting by adding a penalty term to the loss function.</li> <li>Types: L1 regularization (Lasso), L2 regularization (Ridge), Elastic Net.</li> <li>Resource: L1 and L2 Regularization Methods - GeeksforGeeks</li> </ul> </li> <li>Cross-Validation:<ul> <li>Description: A technique to evaluate the performance of a model on unseen data by splitting the data into multiple folds and training/testing the model on different combinations of folds.</li> <li>Types: k-fold cross-validation, stratified cross-validation.</li> <li>Resource: Cross-Validation - Towards Data Science</li> </ul> </li> <li>Evaluation Metrics: (Accuracy, Precision, Recall, F1-Score, AUC, ROC)<ul> <li>Accuracy:  The proportion of correctly classified instances.<ul> <li>Resource: (Basic Statistical concept)</li> </ul> </li> <li>Precision: The proportion of positive predictions that are actually correct.<ul> <li>Resource: (Basic Statistical concept)</li> </ul> </li> <li>Recall: The proportion of actual positive instances that are correctly predicted.<ul> <li>Resource: (Basic Statistical concept)</li> </ul> </li> <li>F1-Score: The harmonic mean of precision and recall.<ul> <li>Resource: (Basic Statistical concept)</li> </ul> </li> <li>AUC (Area Under the Curve):  Measures the ability of a classifier to distinguish between positive and negative classes.<ul> <li>Resource: ROC and AUC, Clearly Explained! - StatQuest</li> </ul> </li> <li>ROC (Receiver Operating Characteristic): A graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied.</li> </ul> </li> </ul>"},{"location":"ml_prep/#ensemble-methods","title":"Ensemble Methods","text":"<ul> <li>Bagging: Training multiple models on bootstrap samples (random samples with replacement) of the training data and averaging their predictions.  Random Forests are a type of bagging.<ul> <li>Resource: Bagging and Boosting - Towards Data Science</li> </ul> </li> <li>Boosting: Sequentially training models, with each model focusing on correcting the errors of previous models.<ul> <li>Examples: AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost.</li> <li>Resource: XGBoost, CatBoost and LightGBM \u2014 Towards Data Science</li> </ul> </li> </ul>"},{"location":"ml_prep/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li>Handling Missing Values: Imputation (mean, median, mode), deletion.</li> <li>Feature Scaling: Standardization (scaling to zero mean and unit variance), Normalization (scaling to a range between 0 and 1).</li> <li>Encoding Categorical Variables: One-Hot Encoding, Label Encoding, Ordinal Encoding.</li> </ul>"},{"location":"ml_prep/#feature-engineering","title":"Feature Engineering","text":"<ul> <li>Creating New Features: Combining existing features, extracting features from text data, etc.</li> <li>Feature Selection: Selecting the most relevant features using statistical tests, feature importance scores, etc.</li> </ul>"},{"location":"ml_prep/#advanced-concepts","title":"Advanced Concepts","text":"<ul> <li>Reinforcement Learning: Training an agent to make decisions in an environment to maximize a reward signal.</li> <li>Deep Learning: Using neural networks with multiple layers to learn complex patterns in data.  (Covered in your Neural Networks section).</li> <li>Time Series Analysis: Analyzing data collected over time to identify patterns and make predictions.</li> <li>Causal Inference: Determining the causal relationships between variables.</li> </ul>"},{"location":"ml_prep/#resources-general","title":"Resources (General)","text":"<ul> <li>Scikit-learn Documentation: https://scikit-learn.org/stable/</li> <li>\"Hands-On Machine Learning with Scikit-Learn, Keras &amp; TensorFlow\" by Aur\u00e9lien G\u00e9ron: A popular textbook with practical examples.</li> <li>\"The Elements of Statistical Learning\" by Hastie, Tibshirani, and Friedman: A classic textbook covering statistical learning theory. (More mathematically rigorous)</li> </ul>"},{"location":"neural_networks/","title":"Neural Networks","text":"<p>This section covers the fundamentals of neural networks and deep learning. The main goal is to have everything to prepare for the most basic interviews</p>"},{"location":"neural_networks/#foundations","title":"Foundations","text":""},{"location":"neural_networks/#what-is-a-neural-network","title":"What is a Neural Network?","text":"<p>A computational model inspired by the structure and function of biological neural networks. Consists of interconnected nodes (\"neurons\") organized in layers that process and transmit information.</p>"},{"location":"neural_networks/#key-components","title":"Key Components","text":"<ul> <li>Neurons (Nodes): The basic processing unit. Receives inputs, applies a weight and bias, and passes the result through an activation function.</li> <li>Weights: Numerical values that represent the strength of the connection between neurons. Learned during training.</li> <li>Bias: An additional parameter added to the weighted sum of inputs. Allows the neuron to activate even when all inputs are zero.</li> <li>Activation Functions: Introduce non-linearity, enabling the network to learn complex patterns.</li> <li>Layers: Organize neurons into input, hidden, and output layers.<ul> <li>Input Layer: Receives the raw input data.</li> <li>Hidden Layers: Perform the primary computations.</li> <li>Output Layer: Produces the final result (e.g., class probabilities).</li> </ul> </li> </ul>"},{"location":"neural_networks/#activation-functions","title":"Activation Functions","text":""},{"location":"neural_networks/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<ul> <li>Description: Output is 0 if input is negative, and equal to the input if positive (f(x) = max(0, x)).</li> <li>Pros: Simple, computationally efficient, helps alleviate the vanishing gradient problem.</li> <li>Cons: Can suffer from the \"dying ReLU\" problem (neurons can get stuck in an inactive state).</li> <li>Reference: Deep Sparse Rectifier Neural Networks (Glorot et al., 2011)</li> </ul>"},{"location":"neural_networks/#sigmoid","title":"Sigmoid","text":"<ul> <li>Description: Output is between 0 and 1 (f(x) = 1 / (1 + exp(-x))).</li> <li>Pros: Easy to interpret as a probability.</li> <li>Cons: Suffers from vanishing gradients (especially in deep networks), not zero-centered, computationally expensive.</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"neural_networks/#tanh-hyperbolic-tangent","title":"Tanh (Hyperbolic Tangent)","text":"<ul> <li>Description: Output is between -1 and 1 (f(x) = tanh(x)).</li> <li>Pros: Zero-centered output (can speed up learning).</li> <li>Cons: Still suffers from vanishing gradients (though less severely than sigmoid).</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"neural_networks/#softmax","title":"Softmax","text":"<ul> <li>Description: Converts a vector of numbers into a probability distribution (used in the output layer for multi-class classification).</li> <li>Pros: Outputs a valid probability distribution, suitable for multi-class problems.</li> <li>Cons: Not suitable for multi-label classification.</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"neural_networks/#loss-functions","title":"Loss Functions","text":""},{"location":"neural_networks/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<ul> <li>Description: Calculates the average squared difference between predicted and actual values (used for regression).</li> <li>Pros: Simple to calculate, differentiable.</li> <li>Cons: Sensitive to outliers.</li> <li>Reference: (Basic statistical concept)</li> </ul>"},{"location":"neural_networks/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<ul> <li>Description: Measures the difference between two probability distributions (predicted vs. actual labels) - used primarily for classification (single or multi-class)</li> <li>Pros: Well-suited for classification, penalizes incorrect predictions more strongly.</li> <li>Cons: Requires probabilistic outputs (e.g., from Softmax).</li> </ul>"},{"location":"neural_networks/#optimization-algorithms","title":"Optimization Algorithms","text":""},{"location":"neural_networks/#gradient-descent","title":"Gradient Descent","text":"<ul> <li>Description: Iteratively adjusts model parameters to minimize the loss function by moving in the direction of the negative gradient.</li> <li>Types: Batch, Stochastic, Mini-Batch<ul> <li>Batch Gradient Descent: Calculates gradient over the entire training set (slow for large datasets).</li> <li>Stochastic Gradient Descent (SGD): Calculates gradient for each individual example (noisy, can escape local minima).</li> <li>Mini-Batch Gradient Descent: Calculates gradient over a small batch of examples (balances speed and stability).</li> </ul> </li> <li>Reference: (Basic optimization concept)</li> </ul>"},{"location":"neural_networks/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<ul> <li>Description: An adaptive learning rate optimization algorithm that combines the ideas of momentum and RMSProp.</li> <li>Pros: Often converges faster and more reliably than SGD, adapts learning rates for each parameter.</li> <li>Cons: More complex than SGD, requires tuning of hyperparameters.</li> <li>Reference: Adam: A Method for Stochastic Optimization (Kingma &amp; Ba, 2014)</li> </ul>"},{"location":"neural_networks/#rmsprop-root-mean-square-propagation","title":"RMSProp (Root Mean Square Propagation)","text":"<ul> <li>Description: An adaptive learning rate method that adjusts the learning rate based on the magnitude of recent gradients.</li> <li>Pros: Handles different learning rates per parameter</li> <li>Cons: Can still make some models get out of balance and overfit</li> <li>Reference: (Hinton lecture slides - unpublished but widely known)</li> </ul>"},{"location":"neural_networks/#regularization-techniques","title":"Regularization Techniques","text":""},{"location":"neural_networks/#l1-and-l2-regularization","title":"L1 and L2 Regularization","text":"<ul> <li>Description: Add a penalty term to the loss function based on the magnitude of the weights. L1 encourages sparsity (some weights become zero), L2 shrinks weights towards zero.</li> <li>Pros: Reduces overfitting by preventing weights from becoming too large.</li> <li>Cons: Requires tuning of the regularization strength (lambda).</li> </ul>"},{"location":"neural_networks/#dropout","title":"Dropout","text":"<ul> <li>Description: Randomly sets a fraction of neuron activations to zero during training.</li> <li>Pros: Reduces overfitting by preventing neurons from becoming too reliant on specific features.</li> <li>Cons: Requires tuning of the dropout rate.</li> </ul>"},{"location":"neural_networks/#handling-imbalanced-datasets","title":"Handling Imbalanced Datasets","text":"<ul> <li>What is It: Imbalanced datasets have a disproportionate ratio of classes.</li> </ul> <p>Techniques include  1. Over Sampling 2. Use Synthetic data 3. Do Under Sampling    Reference: https://www.analyticsvidhya.com/blog/2023/09/8-techniques-to-tackle-imbalanced-data-for-classification/\"\"\"</p>"},{"location":"neural_networks/#resources-additional","title":"Resources (Additional)","text":"<ul> <li>Fast.ai Deep Learning Course: A great practical introduction to deep learning.</li> <li>3Blue1Brown Neural Network Series: Excellent visualizations and intuitive explanations on Youtube.</li> </ul>"},{"location":"neural_networks/#my-notes","title":"My Notes","text":"<p>(Add your own notes, code snippets, and explanations here.)</p> <p>You can download my interview prep notes on Nueral networks Download here</p>"},{"location":"research/","title":"Research Papers","text":"<p>This section links to important research papers in the field.</p>"},{"location":"research/#key-papers","title":"Key Papers","text":"<ul> <li>Attention is All You Need (2017-06-12) - Introduces the Transformer architecture, which is the foundation for modern LLMs.</li> <li>Language Models are Few-Shot Learners (2020-05-28) - Demonstrates the capabilities of large language models for few-shot learning (GPT-3).</li> <li>LoRA: Low-Rank Adaptation of Large Language Models (2021-06-09) - Introduces LoRA, a parameter-efficient fine-tuning technique.</li> <li>QLORA: Efficient Finetuning of Quantized LLMs (2023-05-23) - Introduces QLoRA, a memory-efficient fine-tuning technique combining LoRA with quantization.</li> <li>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022-05-25) - Presents FlashAttention, a technique for efficient attention computation.</li> </ul>"}]}