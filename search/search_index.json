{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Akhilesh's Study Guide","text":"<p>This website is a compilation of resources I'm using to prepare for Machine Learning and AI engineering interviews.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ul> <li>Machine Learing</li> <li>Neural Networks</li> <li>GenAI</li> <li>Research Papers</li> <li>Useful Links</li> </ul>"},{"location":"#about-me","title":"About Me","text":"<p>Machine Learning Engineer specializing in Generative AI. Drawing on a strong foundation in machine learning and data science, I am now focused on building and deploying innovative LLM-powered solutions and creating efficient multimodal data pipelines.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Use the navigation links above to explore different areas. I plan to add more topics over time, so check back often!</p>"},{"location":"#interview-material","title":"Interview Material","text":"<p>See the Interview PDFs and External Resources here</p>"},{"location":"genai/","title":"Generative AI","text":"<p>This section covers Generative AI models, fine-tuning techniques, prompting strategies, and related concepts.</p>"},{"location":"genai/#core-concepts","title":"Core Concepts","text":""},{"location":"genai/#what-is-generative-ai","title":"What is Generative AI?","text":"<p>Generative AI refers to a class of machine learning models that can generate new content, such as text, images, audio, and video. They learn the underlying patterns in training data and can create novel outputs that resemble that data.</p>"},{"location":"genai/#key-types-of-generative-models","title":"Key Types of Generative Models","text":"<ul> <li>Large Language Models (LLMs): Models trained on massive text datasets, capable of generating coherent and contextually relevant text, translations, code, and more. (Examples: GPT, Llama, Gemma, Claude).</li> <li>Diffusion Models: Used primarily for image generation. They learn to reverse a process of gradually adding noise to an image, allowing them to generate new images by starting from random noise and iteratively removing it. (Examples: Stable Diffusion, DALL-E 3, Imagen).</li> <li>Variational Autoencoders (VAEs): A type of neural network that learns a compressed, latent representation of data and can then generate new samples from that latent space.</li> <li>Generative Adversarial Networks (GANs): Consist of two networks, a generator and a discriminator, that compete against each other. The generator tries to create realistic data, while the discriminator tries to distinguish between real and generated data.</li> </ul>"},{"location":"genai/#large-language-models-llms","title":"Large Language Models (LLMs)","text":""},{"location":"genai/#transformer-architecture","title":"Transformer Architecture","text":"<ul> <li>Description: The dominant architecture for LLMs. Uses self-attention mechanisms to weigh the importance of different parts of the input sequence.</li> <li>Key Components: Attention mechanisms, multi-head attention, encoder layers, decoder layers, feedforward networks, residual connections, layer normalization.</li> <li>Key Advantages: Ability to capture long-range dependencies, parallelizable training.</li> <li>Reference: Attention is All You Need (Vaswani et al., 2017)</li> </ul>"},{"location":"genai/#attention-mechanism","title":"Attention Mechanism","text":"<ul> <li>Description: Allows the model to focus on relevant parts of the input when processing each word or token.</li> <li>Process: Calculating query, key, and value vectors; computing attention scores; weighting value vectors based on attention scores.</li> <li>Reference: Attention is All You Need (Vaswani et al., 2017)</li> </ul>"},{"location":"genai/#key-llm-families","title":"Key LLM Families","text":"<ul> <li>GPT (OpenAI): Proprietary models known for strong general-purpose language capabilities, few-shot learning, and scaling.<ul> <li>OpenAI API Documentation</li> </ul> </li> <li>Llama (Meta): Open-source models allowing more transparency, customization, and local deployment.<ul> <li>Meta Llama</li> </ul> </li> <li>Gemma (Google): Open models from Google, designed for responsible AI development and deployment. Focuses on efficiency and performance.<ul> <li>Google AI Gemma</li> </ul> </li> <li>Claude (Anthropic): Proprietary models focused on safety and helpfulness.</li> </ul>"},{"location":"genai/#fine-tuning-techniques","title":"Fine-Tuning Techniques","text":""},{"location":"genai/#supervised-fine-tuning-sft","title":"Supervised Fine-Tuning (SFT)","text":"<ul> <li>Description: Training a pre-trained LLM on a labeled dataset to adapt it to a specific task (e.g., text summarization, question answering).</li> <li>Process: Prepare a dataset of input-output pairs, train the model using a loss function that compares the predicted output to the ground truth output, adjust the model's weights to minimize the loss.</li> </ul>"},{"location":"genai/#parameter-efficient-fine-tuning-peft","title":"Parameter-Efficient Fine-Tuning (PEFT)","text":"<ul> <li>Description: Techniques to reduce the computational cost and memory requirements of fine-tuning large LLMs by only training a small subset of the model's parameters.</li> <li>Key Techniques: LoRA, QLoRA, Adapters.</li> </ul>"},{"location":"genai/#lora-low-rank-adaptation","title":"LoRA (Low-Rank Adaptation)","text":"<ul> <li>Description: Freezes the pre-trained model weights and injects trainable low-rank matrices into each Transformer layer.</li> <li>Advantages: Significantly reduces the number of trainable parameters, faster training, lower memory requirements.</li> <li>Reference: LoRA: Low-Rank Adaptation of Large Language Models (Hu et al., 2021)</li> </ul>"},{"location":"genai/#qlora-quantized-lora","title":"QLoRA (Quantized LoRA)","text":"<ul> <li>Description: Combines LoRA with quantization techniques (e.g., 4-bit quantization) to further reduce memory footprint.</li> <li>Advantages: Extremely memory-efficient, allows fine-tuning of very large models on consumer hardware.</li> <li>Reference: QLORA: Efficient Finetuning of Quantized LLMs (Dettmers et al., 2023)</li> </ul>"},{"location":"genai/#fine-tuning-with-unsloth","title":"Fine-tuning with Unsloth","text":"<ul> <li>Description: Easy way to use QLoRA with speed improvements.<ul> <li>Code Example: <code>python from unsloth import FastLanguageModel model, tokenizer = FastLanguageModel.from_pretrained(     model_name=\"&lt;MODEL_NAME&gt;\", # Replace with your required data location     max_seq_length=2048,     load_in_4bit=True,     )</code></li> </ul> </li> </ul>"},{"location":"genai/#prompt-engineering-techniques","title":"Prompt Engineering Techniques","text":"<ul> <li>Zero-shot Prompting: Providing a prompt without any examples.</li> <li>Few-shot Prompting: Providing a prompt with a few examples to guide the model.</li> <li>Chain of Thought Prompting: Encouraging the model to reason step-by-step to improve complex reasoning tasks.<ul> <li>Reference: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2022)</li> </ul> </li> <li>Pydantic will help creating and validating the inputs and outputs in Pythonic way</li> </ul>"},{"location":"genai/#multimodal-models","title":"Multimodal Models","text":""},{"location":"genai/#architectures","title":"Architectures","text":"<ul> <li>Connectors/Fusion Modules: How vision, audio, and textual representations are combined (attention mechanisms, cross-attention).</li> <li>Training Objectives: How these models are trained to align different modalities.</li> </ul>"},{"location":"genai/#models","title":"Models","text":"<ul> <li>Gemini (Google): Google Documentation</li> <li>Stable Diffusion: Stable Diffusion Website</li> </ul>"},{"location":"genai/#deployment-and-optimization","title":"Deployment and Optimization","text":""},{"location":"genai/#quantization","title":"Quantization","text":"<ul> <li>Description: Reducing the precision of model weights (e.g., from 32-bit floats to 8-bit integers) to reduce model size and improve inference speed.</li> <li>Techniques: Post-Training Quantization, Quantization-Aware Training (QAT)</li> </ul>"},{"location":"genai/#knowledge-distillation","title":"Knowledge Distillation","text":"<ul> <li>Description: Training a smaller \"student\" model to mimic the behavior of a larger \"teacher\" model.</li> <li>Pros: Reduces model size and improves inference speed.</li> </ul>"},{"location":"genai/#my-notes","title":"My Notes","text":"<p>(Add your own notes, code snippets, and explanations here.)</p> <p>Here's a good resource on prompts to start with in LLM section: Prompt Engineering Guide</p>"},{"location":"ml_prep/","title":"Machine Learning Preparation","text":"<p>This section contains resources for general machine learning concepts, algorithms, and data structures.</p>"},{"location":"ml_prep/#algorithms","title":"Algorithms","text":"<ul> <li>Linear Regression: [Link to a good explanation]</li> <li>Logistic Regression: [Link to another explanation]</li> <li>Decision Trees: [Link]</li> <li>Support Vector Machines (SVMs): [Link]</li> <li>K-Nearest Neighbors (KNN): [Link]</li> <li>Clustering:<ul> <li>K-Means: [Link]</li> <li>Hierarchical Clustering: [Link]</li> </ul> </li> <li>Dimensionality Reduction:<ul> <li>PCA: [Link]</li> <li>t-SNE: [Link]</li> </ul> </li> </ul>"},{"location":"ml_prep/#data-structures","title":"Data Structures","text":"<ul> <li>Arrays: [Link]</li> <li>Linked Lists: [Link]</li> <li>Trees: [Link]</li> <li>Graphs: [Link]</li> <li>Hash Tables: [Link]</li> </ul>"},{"location":"ml_prep/#key-concepts","title":"Key Concepts","text":"<ul> <li>Bias-Variance Tradeoff: [Link]</li> <li>Overfitting/Underfitting: [Link]</li> <li>Regularization: [Link]</li> <li>Cross-Validation: [Link]</li> <li>Evaluation Metrics: (Accuracy, Precision, Recall, F1-Score, AUC): [Link]</li> </ul>"},{"location":"neural_networks/","title":"Neural Networks","text":"<p>This section covers the fundamentals of neural networks and deep learning. The main goal is to have everything to prepare for the most basic interviews</p>"},{"location":"neural_networks/#foundations","title":"Foundations","text":""},{"location":"neural_networks/#what-is-a-neural-network","title":"What is a Neural Network?","text":"<p>A computational model inspired by the structure and function of biological neural networks. Consists of interconnected nodes (\"neurons\") organized in layers that process and transmit information.</p>"},{"location":"neural_networks/#key-components","title":"Key Components","text":"<ul> <li>Neurons (Nodes): The basic processing unit. Receives inputs, applies a weight and bias, and passes the result through an activation function.</li> <li>Weights: Numerical values that represent the strength of the connection between neurons. Learned during training.</li> <li>Bias: An additional parameter added to the weighted sum of inputs. Allows the neuron to activate even when all inputs are zero.</li> <li>Activation Functions: Introduce non-linearity, enabling the network to learn complex patterns.</li> <li>Layers: Organize neurons into input, hidden, and output layers.<ul> <li>Input Layer: Receives the raw input data.</li> <li>Hidden Layers: Perform the primary computations.</li> <li>Output Layer: Produces the final result (e.g., class probabilities).</li> </ul> </li> </ul>"},{"location":"neural_networks/#activation-functions","title":"Activation Functions","text":""},{"location":"neural_networks/#relu-rectified-linear-unit","title":"ReLU (Rectified Linear Unit)","text":"<ul> <li>Description: Output is 0 if input is negative, and equal to the input if positive (f(x) = max(0, x)).</li> <li>Pros: Simple, computationally efficient, helps alleviate the vanishing gradient problem.</li> <li>Cons: Can suffer from the \"dying ReLU\" problem (neurons can get stuck in an inactive state).</li> <li>Reference: Deep Sparse Rectifier Neural Networks (Glorot et al., 2011)</li> </ul>"},{"location":"neural_networks/#sigmoid","title":"Sigmoid","text":"<ul> <li>Description: Output is between 0 and 1 (f(x) = 1 / (1 + exp(-x))).</li> <li>Pros: Easy to interpret as a probability.</li> <li>Cons: Suffers from vanishing gradients (especially in deep networks), not zero-centered, computationally expensive.</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"neural_networks/#tanh-hyperbolic-tangent","title":"Tanh (Hyperbolic Tangent)","text":"<ul> <li>Description: Output is between -1 and 1 (f(x) = tanh(x)).</li> <li>Pros: Zero-centered output (can speed up learning).</li> <li>Cons: Still suffers from vanishing gradients (though less severely than sigmoid).</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"neural_networks/#softmax","title":"Softmax","text":"<ul> <li>Description: Converts a vector of numbers into a probability distribution (used in the output layer for multi-class classification).</li> <li>Pros: Outputs a valid probability distribution, suitable for multi-class problems.</li> <li>Cons: Not suitable for multi-label classification.</li> <li>Reference: (Commonly used, no single defining paper)</li> </ul>"},{"location":"neural_networks/#loss-functions","title":"Loss Functions","text":""},{"location":"neural_networks/#mean-squared-error-mse","title":"Mean Squared Error (MSE)","text":"<ul> <li>Description: Calculates the average squared difference between predicted and actual values (used for regression).</li> <li>Pros: Simple to calculate, differentiable.</li> <li>Cons: Sensitive to outliers.</li> <li>Reference: (Basic statistical concept)</li> </ul>"},{"location":"neural_networks/#cross-entropy-loss","title":"Cross-Entropy Loss","text":"<ul> <li>Description: Measures the difference between two probability distributions (predicted vs. actual labels) - used primarily for classification (single or multi-class)</li> <li>Pros: Well-suited for classification, penalizes incorrect predictions more strongly.</li> <li>Cons: Requires probabilistic outputs (e.g., from Softmax).</li> </ul>"},{"location":"neural_networks/#optimization-algorithms","title":"Optimization Algorithms","text":""},{"location":"neural_networks/#gradient-descent","title":"Gradient Descent","text":"<ul> <li>Description: Iteratively adjusts model parameters to minimize the loss function by moving in the direction of the negative gradient.</li> <li>Types: Batch, Stochastic, Mini-Batch<ul> <li>Batch Gradient Descent: Calculates gradient over the entire training set (slow for large datasets).</li> <li>Stochastic Gradient Descent (SGD): Calculates gradient for each individual example (noisy, can escape local minima).</li> <li>Mini-Batch Gradient Descent: Calculates gradient over a small batch of examples (balances speed and stability).</li> </ul> </li> <li>Reference: (Basic optimization concept)</li> </ul>"},{"location":"neural_networks/#adam-adaptive-moment-estimation","title":"Adam (Adaptive Moment Estimation)","text":"<ul> <li>Description: An adaptive learning rate optimization algorithm that combines the ideas of momentum and RMSProp.</li> <li>Pros: Often converges faster and more reliably than SGD, adapts learning rates for each parameter.</li> <li>Cons: More complex than SGD, requires tuning of hyperparameters.</li> <li>Reference: Adam: A Method for Stochastic Optimization (Kingma &amp; Ba, 2014)</li> </ul>"},{"location":"neural_networks/#rmsprop-root-mean-square-propagation","title":"RMSProp (Root Mean Square Propagation)","text":"<ul> <li>Description: An adaptive learning rate method that adjusts the learning rate based on the magnitude of recent gradients.</li> <li>Pros: Handles different learning rates per parameter</li> <li>Cons: Can still make some models get out of balance and overfit</li> <li>Reference: (Hinton lecture slides - unpublished but widely known)</li> </ul>"},{"location":"neural_networks/#regularization-techniques","title":"Regularization Techniques","text":""},{"location":"neural_networks/#l1-and-l2-regularization","title":"L1 and L2 Regularization","text":"<ul> <li>Description: Add a penalty term to the loss function based on the magnitude of the weights. L1 encourages sparsity (some weights become zero), L2 shrinks weights towards zero.</li> <li>Pros: Reduces overfitting by preventing weights from becoming too large.</li> <li>Cons: Requires tuning of the regularization strength (lambda).</li> </ul>"},{"location":"neural_networks/#dropout","title":"Dropout","text":"<ul> <li>Description: Randomly sets a fraction of neuron activations to zero during training.</li> <li>Pros: Reduces overfitting by preventing neurons from becoming too reliant on specific features.</li> <li>Cons: Requires tuning of the dropout rate.</li> </ul>"},{"location":"neural_networks/#handling-imbalanced-datasets","title":"Handling Imbalanced Datasets","text":"<ul> <li>What is It: Imbalanced datasets have a disproportionate ratio of classes.</li> </ul> <p>Techniques include  1. Over Sampling 2. Use Synthetic data 3. Do Under Sampling    Reference: https://www.analyticsvidhya.com/blog/2023/09/8-techniques-to-tackle-imbalanced-data-for-classification/\"\"\"</p>"},{"location":"neural_networks/#resources-additional","title":"Resources (Additional)","text":"<ul> <li>Fast.ai Deep Learning Course: A great practical introduction to deep learning.</li> <li>3Blue1Brown Neural Network Series: Excellent visualizations and intuitive explanations on Youtube.</li> </ul>"},{"location":"neural_networks/#my-notes","title":"My Notes","text":"<p>(Add your own notes, code snippets, and explanations here.)</p> <p>You can download my interview prep notes on Nueral networks Download here</p>"},{"location":"research/","title":"Research Papers","text":"<p>This section links to important research papers in the field.</p>"},{"location":"research/#key-papers","title":"Key Papers","text":"<ul> <li>Attention is All You Need (2017-06-12) - Introduces the Transformer architecture, which is the foundation for modern LLMs.</li> <li>Language Models are Few-Shot Learners (2020-05-28) - Demonstrates the capabilities of large language models for few-shot learning (GPT-3).</li> <li>LoRA: Low-Rank Adaptation of Large Language Models (2021-06-09) - Introduces LoRA, a parameter-efficient fine-tuning technique.</li> <li>QLORA: Efficient Finetuning of Quantized LLMs (2023-05-23) - Introduces QLoRA, a memory-efficient fine-tuning technique combining LoRA with quantization.</li> <li>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022-05-25) - Presents FlashAttention, a technique for efficient attention computation.</li> </ul>"},{"location":"research/#my-notes","title":"My Notes","text":"<p>(Add your own notes, code snippets, and explanations here.)</p>"}]}